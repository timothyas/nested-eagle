---
defaults:
  - plot: detailed
  - callbacks: placeholder
  - benchmark_profiler: detailed

# another alternative if you don't have any callbacks is to remove it from the
# defaults list and just use
# callbacks: []

debug:
  # this will detect and trace back NaNs / Infs etc. but will slow down training
  anomaly_detection: False

enable_checkpointing: True
checkpoint:
  every_n_minutes:
    save_frequency: 30 # Approximate, as this is checked at the end of training steps
    num_models_saved: 3 # If set to k, saves the 'last' k model weights in the training.

  every_n_epochs:
    save_frequency: 1
    num_models_saved: -1 # If set to -1, all checkpoints are kept ensuring runs can be continued/forked at any point in the training process

  every_n_train_steps:
    save_frequency: null # Does not scale with rollout
    num_models_saved: 0

log:
  wandb:
    enabled: False
    offline: False
    log_model: False
    project: 'Anemoi'
    entity: example
    # logger options (these probably come with some overhead)
    gradients: False
    parameters: False
  tensorboard:
    enabled: False
  mlflow:
    enabled: False
    _target_: anemoi.training.diagnostics.mlflow.logger.AnemoiMLflowLogger
    offline: False
    authentication: False
    tracking_uri: ???
    experiment_name: 'anemoi-debug'
    project_name: 'Anemoi'
    system: False
    terminal: True
    run_name: null # If set to null, the run name will be the a random UUID
    on_resume_create_child: True
    expand_hyperparams: # Which keys in hyperparams to expand
      - config
    http_max_retries: 35
    max_params_length: 2000
    save_dir: ${system.output.logs.mlflow}
  interval: 100 # passed to trainer.log_every_n_steps

enable_progress_bar: True
progress_bar:
  _target_: pytorch_lightning.callbacks.TQDMProgressBar
  refresh_rate: 1

check_val_every_n_epoch: 1
print_memory_summary: False
